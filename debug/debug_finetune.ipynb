{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0a728e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"/home/ziniuw/zero-shot-cost-estimation\")\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "from cross_db_benchmark.benchmark_tools.database import DatabaseSystem\n",
    "from models.training.train import train_default, train_readout_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from models.dataset.dataset_creation import create_dataloader\n",
    "from models.training.checkpoint import save_checkpoint, load_checkpoint, save_csv\n",
    "from models.training.metrics import MAPE, RMSE, QError\n",
    "from models.training.utils import batch_to, flatten_dict, find_early_stopping_metric\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "\n",
    "def training_model_loader(workload_runs,\n",
    "                test_workload_runs,\n",
    "                statistics_file,\n",
    "                target_dir,\n",
    "                filename_model,\n",
    "                optimizer_class_name='Adam',\n",
    "                optimizer_kwargs=None,\n",
    "                final_mlp_kwargs=None,\n",
    "                node_type_kwargs=None,\n",
    "                model_kwargs=None,\n",
    "                tree_layer_name='GATConv',\n",
    "                tree_layer_kwargs=None,\n",
    "                hidden_dim=32,\n",
    "                batch_size=32,\n",
    "                output_dim=1,\n",
    "                epochs=0,\n",
    "                device='cpu',\n",
    "                plan_featurization_name=None,\n",
    "                max_epoch_tuples=100000,\n",
    "                param_dict=None,\n",
    "                num_workers=1,\n",
    "                early_stopping_patience=20,\n",
    "                trial=None,\n",
    "                database=None,\n",
    "                limit_queries=None,\n",
    "                limit_queries_affected_wl=None,\n",
    "                skip_train=False,\n",
    "                seed=0):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = dict()\n",
    "\n",
    "    # seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    target_test_csv_paths = []\n",
    "    if test_workload_runs is not None:\n",
    "        for p in test_workload_runs:\n",
    "            test_workload = os.path.basename(p).replace('.json', '')\n",
    "            target_test_csv_paths.append(os.path.join(target_dir, f'test_{filename_model}_{test_workload}.csv'))\n",
    "\n",
    "    # create a dataset\n",
    "    loss_class_name = final_mlp_kwargs['loss_class_name']\n",
    "    label_norm, feature_statistics, train_loader, val_loader, test_loaders = \\\n",
    "        create_dataloader(workload_runs, test_workload_runs, statistics_file, plan_featurization_name, database,\n",
    "                          val_ratio=0.15, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                          pin_memory=False, limit_queries=limit_queries,\n",
    "                          limit_queries_affected_wl=limit_queries_affected_wl, loss_class_name=loss_class_name)\n",
    "\n",
    "    if loss_class_name == 'QLoss':\n",
    "        metrics = [RMSE(), MAPE(), QError(percentile=50, early_stopping_metric=True), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "    elif loss_class_name == 'MSELoss':\n",
    "        metrics = [RMSE(early_stopping_metric=True), MAPE(), QError(percentile=50), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "\n",
    "    # create zero shot model dependent on database\n",
    "    model = zero_shot_models[database](device=device, hidden_dim=hidden_dim, final_mlp_kwargs=final_mlp_kwargs,\n",
    "                                       node_type_kwargs=node_type_kwargs, output_dim=output_dim,\n",
    "                                       feature_statistics=feature_statistics, tree_layer_name=tree_layer_name,\n",
    "                                       tree_layer_kwargs=tree_layer_kwargs,\n",
    "                                       plan_featurization_name=plan_featurization_name,\n",
    "                                       label_norm=label_norm,\n",
    "                                       **model_kwargs)\n",
    "    # move to gpu\n",
    "    model = model.to(model.device)\n",
    "    optimizer = opt.__dict__[optimizer_class_name](model.parameters(), **optimizer_kwargs)\n",
    "    csv_stats, epochs_wo_improvement, epoch, model, optimizer, metrics, finished = \\\n",
    "        load_checkpoint(model, target_dir, filename_model, optimizer=optimizer, metrics=metrics, filetype='.pt')\n",
    "    return test_loaders, model\n",
    "\n",
    "def load_model(workload_runs,\n",
    "            test_workload_runs,\n",
    "               statistics_file,\n",
    "               target_dir,\n",
    "               filename_model,\n",
    "               hyperparameter_path,\n",
    "              device='cpu',\n",
    "              max_epoch_tuples=100000,\n",
    "              num_workers=1,\n",
    "              loss_class_name='QLoss',\n",
    "              database=None,\n",
    "              seed=0,\n",
    "              limit_queries=None,\n",
    "              limit_queries_affected_wl=None,\n",
    "              max_no_epochs=None,\n",
    "              skip_train=False\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Reads out hyperparameters and trains model\n",
    "    \"\"\"\n",
    "    print(f\"Reading hyperparameters from {hyperparameter_path}\")\n",
    "    hyperparams = load_json(hyperparameter_path, namespace=False)\n",
    "\n",
    "    p_dropout = hyperparams.pop('p_dropout')\n",
    "    # general fc out\n",
    "    fc_out_kwargs = dict(p_dropout=p_dropout,\n",
    "                         activation_class_name='LeakyReLU',\n",
    "                         activation_class_kwargs={},\n",
    "                         norm_class_name='Identity',\n",
    "                         norm_class_kwargs={},\n",
    "                         residual=hyperparams.pop('residual'),\n",
    "                         dropout=hyperparams.pop('dropout'),\n",
    "                         activation=True,\n",
    "                         inplace=True)\n",
    "    final_mlp_kwargs = dict(width_factor=hyperparams.pop('final_width_factor'),\n",
    "                            n_layers=hyperparams.pop('final_layers'),\n",
    "                            loss_class_name=loss_class_name,\n",
    "                            loss_class_kwargs=dict())\n",
    "    tree_layer_kwargs = dict(width_factor=hyperparams.pop('tree_layer_width_factor'),\n",
    "                             n_layers=hyperparams.pop('message_passing_layers'))\n",
    "    node_type_kwargs = dict(width_factor=hyperparams.pop('node_type_width_factor'),\n",
    "                            n_layers=hyperparams.pop('node_layers'),\n",
    "                            one_hot_embeddings=True,\n",
    "                            max_emb_dim=hyperparams.pop('max_emb_dim'),\n",
    "                            drop_whole_embeddings=False)\n",
    "    final_mlp_kwargs.update(**fc_out_kwargs)\n",
    "    tree_layer_kwargs.update(**fc_out_kwargs)\n",
    "    node_type_kwargs.update(**fc_out_kwargs)\n",
    "\n",
    "    train_kwargs = dict(optimizer_class_name='AdamW',\n",
    "                        optimizer_kwargs=dict(\n",
    "                            lr=hyperparams.pop('lr'),\n",
    "                        ),\n",
    "                        final_mlp_kwargs=final_mlp_kwargs,\n",
    "                        node_type_kwargs=node_type_kwargs,\n",
    "                        tree_layer_kwargs=tree_layer_kwargs,\n",
    "                        tree_layer_name=hyperparams.pop('tree_layer_name'),\n",
    "                        plan_featurization_name=hyperparams.pop('plan_featurization_name'),\n",
    "                        hidden_dim=hyperparams.pop('hidden_dim'),\n",
    "                        output_dim=1,\n",
    "                        epochs=200 if max_no_epochs is None else max_no_epochs,\n",
    "                        early_stopping_patience=20,\n",
    "                        max_epoch_tuples=max_epoch_tuples,\n",
    "                        batch_size=hyperparams.pop('batch_size'),\n",
    "                        device=device,\n",
    "                        num_workers=num_workers,\n",
    "                        seed=seed,\n",
    "                        limit_queries=limit_queries,\n",
    "                        limit_queries_affected_wl=limit_queries_affected_wl,\n",
    "                        skip_train=skip_train\n",
    "                        )\n",
    "\n",
    "    assert len(hyperparams) == 0, f\"Not all hyperparams were used (not used: {hyperparams.keys()}). Hence generation \" \\\n",
    "                                  f\"and reading does not seem to fit\"\n",
    "\n",
    "    param_dict = flatten_dict(train_kwargs)\n",
    "    \n",
    "    test_loaders, model = training_model_loader(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                          param_dict=param_dict, database=database, **train_kwargs)\n",
    "    \n",
    "    return test_loaders, model\n",
    "\n",
    "def validate_model(val_loader, model, epoch=0, epoch_stats=None, metrics=None, max_epoch_tuples=None,\n",
    "                   custom_batch_to=batch_to, verbose=False, log_all_queries=False):\n",
    "    model.eval()\n",
    "    print(model.plan_featurization_name)\n",
    "    with torch.autograd.no_grad():\n",
    "        val_loss = torch.Tensor([0])\n",
    "        labels = []\n",
    "        preds = []\n",
    "        probs = []\n",
    "        sample_idxs = []\n",
    "\n",
    "        # evaluate test set using model\n",
    "        test_start_t = time.perf_counter()\n",
    "        val_num_tuples = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader)):\n",
    "            if max_epoch_tuples is not None and batch_idx * val_loader.batch_size > max_epoch_tuples:\n",
    "                break\n",
    "            val_num_tuples += val_loader.batch_size\n",
    "\n",
    "            input_model, label, sample_idxs_batch = custom_batch_to(batch, model.device, model.label_norm)\n",
    "            #sample_idxs += sample_idxs_batch\n",
    "            output = model(input_model)\n",
    "\n",
    "            # sum up mean batch losses\n",
    "            val_loss += model.loss_fxn(output, label).cpu()\n",
    "\n",
    "            # inverse transform the predictions and labels\n",
    "            curr_pred = output.cpu().numpy()\n",
    "            curr_label = label.cpu().numpy()\n",
    "            if model.label_norm is not None:\n",
    "                curr_pred = model.label_norm.inverse_transform(curr_pred)\n",
    "                curr_label = model.label_norm.inverse_transform(curr_label.reshape(-1, 1))\n",
    "                curr_label = curr_label.reshape(-1)\n",
    "           \n",
    "            preds.append(curr_pred.reshape(-1))\n",
    "            labels.append(curr_label.reshape(-1))\n",
    "\n",
    "        if epoch_stats is not None:\n",
    "            epoch_stats.update(val_time=time.perf_counter() - test_start_t)\n",
    "            epoch_stats.update(val_num_tuples=val_num_tuples)\n",
    "            val_loss = (val_loss.cpu() / len(val_loader)).item()\n",
    "            print(f'val_loss epoch {epoch}: {val_loss}')\n",
    "            epoch_stats.update(val_loss=val_loss)\n",
    "\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0332eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_model(target_dir, filename_model, hyperparameter_path, test_workload_runs=\n",
    "                   [\"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_testing_2k.json\"],\n",
    "                   statistics_file=\"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"):\n",
    "    database = DatabaseSystem.POSTGRES\n",
    "    test_loaders, model = load_model(test_workload_runs, test_workload_runs, statistics_file, target_dir, filename_model,\n",
    "                  hyperparameter_path, database=database)\n",
    "    true, pred = validate_model(test_loaders[0], model)\n",
    "    qerror = np.maximum(true/pred, pred/true)\n",
    "    res = []\n",
    "    for i in [50, 95, 99]:\n",
    "        print(f\"{i} percentile is {np.percentile(qerror, i)}\")\n",
    "        res.append(np.percentile(qerror, i))\n",
    "    return res, true, pred\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "712c8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "statistics_file = \"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "feature_statistics = load_json(statistics_file, namespace=False)\n",
    "dd_statistics_file = \"/home/ziniuw/zero-shot-data/runs/deepdb_augmented/statistics_workload_combined.json\"\n",
    "dd_feature_statistics = load_json(dd_statistics_file, namespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73ee2668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['op_name', 'est_startup_cost', 'est_cost', 'est_card', 'est_width', 'act_startup_cost', 'act_time', 'act_card', 'aggregation', 'columns', 'act_children_card', 'est_children_card', 'workers_planned', 'est_pg', 'est_deepdb', 'dd_est_card', 'dd_est_children_card', 'table', 'column', 'operator', 'literal', 'literal_feature', 'plan_runtime', 'inner_unique', 'join_conds', 'tablename', 'attname', 'null_frac', 'avg_width', 'n_distinct', 'correlation', 'data_type', 'table_size', 'relname', 'reltuples', 'relpages', 'hardware'])\n",
      "{'max': 17.0, 'scale': 5.0, 'center': 4.0, 'type': 'numeric'}\n",
      "{'max': 18306840920064.0, 'scale': 221744.0, 'center': 4000.0, 'type': 'numeric'}\n",
      "{'max': 1.9534922158964736e+16, 'scale': 487599.0, 'center': 2.0, 'type': 'numeric'}\n"
     ]
    }
   ],
   "source": [
    "print(dd_feature_statistics.keys())\n",
    "print(dd_feature_statistics['est_deepdb'])\n",
    "print(dd_feature_statistics['dd_est_card'])\n",
    "print(dd_feature_statistics['dd_est_children_card'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78d47800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max': 113712128000.0, 'scale': 145450.0, 'center': 2500.0, 'type': 'numeric'}\n",
      "{'max': 133562480.0, 'scale': 154997.0, 'center': 3312.0, 'type': 'numeric'}\n",
      "{'max': 86174366433280.0, 'scale': 310558.0, 'center': 2.0, 'type': 'numeric'}\n",
      "{'max': 805373543972864.0, 'scale': 329372.0, 'center': 4.0, 'type': 'numeric'}\n"
     ]
    }
   ],
   "source": [
    "print(feature_statistics['est_card'])\n",
    "print(feature_statistics['act_card'])\n",
    "print(feature_statistics['est_children_card'])\n",
    "print(feature_statistics['act_children_card'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9684b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_statistics['est_mscn'] = {'max': 1217.0, 'scale': 115.0, 'center': 4.0, 'type': 'numeric'}\n",
    "feature_statistics['cc_est_card'] = feature_statistics['act_card']\n",
    "feature_statistics['cc_est_children_card'] = feature_statistics['act_children_card']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b81a9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/ziniuw/zero-shot-data/runs/MSCN_augmented/statistics_workload_combined.json', 'w') as f:\n",
    "    json.dump(feature_statistics,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00770066",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_script = \"python3 train.py --train_model --workload_runs ../zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_training_50k.json --target ../zero-shot-data/evaluation/job_full_few_shot_tune/ --hyperparameter_path setup/tuned_hyperparameters/tune_{}best_config.json --seed 0 --limit_queries {} --limit_queries_affected_wl 1 --filename_model imdb_full_{}lmtq_{} --save_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c12090",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_query_list = [1000, 5000, 10000, 50000]\n",
    "train_with_list = [\"\", \"est_\"]\n",
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_few_shot_tune/\"\n",
    "hyperparameter_paths = {\n",
    "    \"True Card\": \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\",\n",
    "    \"PG_est Card\": \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\"\n",
    "}\n",
    "\n",
    "\n",
    "for num_query in num_query_list:\n",
    "    for train_with in train_with_list:\n",
    "        curr_train_script = training_script.format(train_with, num_query, train_with, num_query)\n",
    "        filename_model = f\"imdb_full_{train_with}lmtq_{num_query}\"\n",
    "        if os.path.exists(target_dir + filename_model + \".pt\"):\n",
    "            for test_with in hyperparameter_paths:\n",
    "                hyperparameter_path = hyperparameter_paths[test_with]\n",
    "                print(\"***************************************************************\")\n",
    "                if train_with == \"\":\n",
    "                    train_with = \"True Card\"\n",
    "                else:\n",
    "                    train_with = \"PG est Card\"\n",
    "                print(f\"evaluting model with {num_query} queries and {train_with}, testing with {test_with}:\")\n",
    "                res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path)\n",
    "        else:\n",
    "            print(\"***************************************************************\")\n",
    "            print(f\"Model with {num_query} queries and {train_with} has not yet begin train, train with:\")\n",
    "            print(curr_train_script)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4bc60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 1598\n",
      "No of Plans: 1598\n",
      "Successfully loaded checkpoint from epoch 85 (85 csv rows) in 1.087 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [01:12<00:00, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2084041833877563\n",
      "95 percentile is 2.4420575022697437\n",
      "99 percentile is 3.2894937086105345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_tune\"\n",
    "filename_model = \"imdb_full_0\"\n",
    "test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/MSCN_augmented/imdb_full/complex_queries_testing_2k.json\"]\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\"\n",
    "res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path, test_workload_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c9654e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 1598\n",
      "No of Plans: 1598\n",
      "Successfully loaded checkpoint from epoch 26 (26 csv rows) in 1.604 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [01:09<00:00,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.3038760423660278\n",
      "95 percentile is 2.7281991481780996\n",
      "99 percentile is 3.827727508544918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_tune\"\n",
    "filename_model = \"imdb_full_0_pg_est\"\n",
    "test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/MSCN_augmented/imdb_full/complex_queries_testing_2k.json\"]\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\"\n",
    "res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path, test_workload_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cff970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_cc_est_best_config.json\n",
      "No of Plans: 1598\n",
      "No of Plans: 1598\n",
      "Successfully loaded checkpoint from epoch 85 (85 csv rows) in 1.019 secs\n",
      "PostgresCardCorrectorDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [01:15<00:00, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.3166757225990295\n",
      "95 percentile is 2.936788487434387\n",
      "99 percentile is 4.018758296966553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_tune\"\n",
    "filename_model = \"imdb_full_0\"\n",
    "test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/MSCN_augmented/imdb_full/complex_queries_testing_2k.json\"]\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_cc_est_best_config.json\"\n",
    "statistics_file=\"/home/ziniuw/zero-shot-data/runs/MSCN_augmented/statistics_workload_combined.json\"\n",
    "res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path, test_workload_runs, statistics_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 1000, 5000, 10000, 50000]\n",
    "true_true = [1.26, 1.26, 1.26, 1.26, 1.26]\n",
    "true_true_ft = [1.26, 1.27, 1.25, 1.27, 1.24]\n",
    "true_est = [1.61, 1.61, 1.61, 1.61, 1.61]\n",
    "true_est_ft = [1.61, 1.57, 1.62, 1.58, 1.63]\n",
    "est_true = [1.46, 1.46, 1.46, 1.46, 1.46]\n",
    "est_true_ft = [1.46, 1.47, 1.35, 1.35, 1.41]\n",
    "est_est = [1.55, 1.55, 1.55, 1.55, 1.55]\n",
    "est_est_ft = [1.55, 1.48, 1.36, 1.31, 1.29]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, true_true, color=\"red\")\n",
    "plt.plot(x, true_true_ft, color=\"red\", linestyle='dashed')\n",
    "plt.scatter(x, true_true_ft, color=\"red\", s=50)\n",
    "\n",
    "plt.plot(x, true_est, color=\"orange\")\n",
    "plt.plot(x, true_est_ft, color=\"orange\", linestyle='dashed')\n",
    "plt.scatter(x, true_est_ft, color=\"orange\", s=50)\n",
    "\n",
    "plt.plot(x, est_true, color=\"blue\")\n",
    "plt.plot(x, est_true_ft, color=\"blue\", linestyle='dashed')\n",
    "plt.scatter(x, est_true_ft, color=\"blue\", s=50)\n",
    "\n",
    "plt.plot(x, est_est, color=\"black\")\n",
    "plt.plot(x, est_est_ft, color=\"black\", linestyle='dashed')\n",
    "plt.scatter(x, est_est_ft, color=\"black\", s=50)\n",
    "\n",
    "plt.ylim(1.2, 1.7)\n",
    "plt.ylabel(\"Median Qerror\", fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Training Queries\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.savefig(\"jobfull.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e79752",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 1000, 5000, 10000, 50000]\n",
    "true_true = [1.26, 1.26, 1.26, 1.26, 1.26]\n",
    "true_true_ft = [1.26, 1.27, 1.25, 1.27, 1.24]\n",
    "true_est = [1.61, 1.61, 1.61, 1.61, 1.61]\n",
    "true_est_ft = [1.61, 1.57, 1.62, 1.58, 1.63]\n",
    "est_true = [1.46, 1.46, 1.46, 1.46, 1.46]\n",
    "est_true_ft = [1.46, 1.47, 1.35, 1.35, 1.41]\n",
    "est_est = [1.55, 1.55, 1.55, 1.55, 1.55]\n",
    "est_est_ft = [1.55, 1.48, 1.36, 1.31, 1.29]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, true_true, color=\"red\")\n",
    "plt.plot(x, true_true_ft, color=\"red\", linestyle='dashed')\n",
    "plt.scatter(x, true_true_ft, color=\"red\", s=50)\n",
    "\n",
    "#plt.plot(x, true_est, color=\"orange\")\n",
    "#plt.plot(x, true_est_ft, color=\"orange\", linestyle='dashed')\n",
    "#plt.scatter(x, true_est_ft, color=\"orange\", s=50)\n",
    "\n",
    "#plt.plot(x, est_true, color=\"blue\")\n",
    "#plt.plot(x, est_true_ft, color=\"blue\", linestyle='dashed')\n",
    "#plt.scatter(x, est_true_ft, color=\"blue\", s=50)\n",
    "\n",
    "plt.plot(x, est_est, color=\"black\")\n",
    "plt.plot(x, est_est_ft, color=\"black\", linestyle='dashed')\n",
    "plt.scatter(x, est_est_ft, color=\"black\", s=50)\n",
    "\n",
    "plt.ylim(1.2, 1.7)\n",
    "plt.ylabel(\"Median Qerror\", fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Training Queries\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.savefig(\"jobfull.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e313b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
