{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0a728e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(\"/home/ziniuw/zero-shot-cost-estimation\")\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "from cross_db_benchmark.benchmark_tools.database import DatabaseSystem\n",
    "from models.training.train import train_default, train_readout_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba5218e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from models.dataset.dataset_creation import create_dataloader\n",
    "from models.training.checkpoint import save_checkpoint, load_checkpoint, save_csv\n",
    "from models.training.metrics import MAPE, RMSE, QError\n",
    "from models.training.utils import batch_to, flatten_dict, find_early_stopping_metric\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "\n",
    "def training_model_loader(workload_runs,\n",
    "                test_workload_runs,\n",
    "                statistics_file,\n",
    "                target_dir,\n",
    "                filename_model,\n",
    "                optimizer_class_name='Adam',\n",
    "                optimizer_kwargs=None,\n",
    "                final_mlp_kwargs=None,\n",
    "                node_type_kwargs=None,\n",
    "                model_kwargs=None,\n",
    "                tree_layer_name='GATConv',\n",
    "                tree_layer_kwargs=None,\n",
    "                hidden_dim=32,\n",
    "                batch_size=32,\n",
    "                output_dim=1,\n",
    "                epochs=0,\n",
    "                device='cpu',\n",
    "                plan_featurization_name=None,\n",
    "                max_epoch_tuples=100000,\n",
    "                param_dict=None,\n",
    "                num_workers=1,\n",
    "                early_stopping_patience=20,\n",
    "                trial=None,\n",
    "                database=None,\n",
    "                limit_queries=None,\n",
    "                limit_queries_affected_wl=None,\n",
    "                skip_train=False,\n",
    "                seed=0):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = dict()\n",
    "\n",
    "    # seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    target_test_csv_paths = []\n",
    "    if test_workload_runs is not None:\n",
    "        for p in test_workload_runs:\n",
    "            test_workload = os.path.basename(p).replace('.json', '')\n",
    "            target_test_csv_paths.append(os.path.join(target_dir, f'test_{filename_model}_{test_workload}.csv'))\n",
    "\n",
    "    # create a dataset\n",
    "    loss_class_name = final_mlp_kwargs['loss_class_name']\n",
    "    label_norm, feature_statistics, train_loader, val_loader, test_loaders = \\\n",
    "        create_dataloader(workload_runs, test_workload_runs, statistics_file, plan_featurization_name, database,\n",
    "                          val_ratio=0.15, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                          pin_memory=False, limit_queries=limit_queries,\n",
    "                          limit_queries_affected_wl=limit_queries_affected_wl, loss_class_name=loss_class_name)\n",
    "\n",
    "    if loss_class_name == 'QLoss':\n",
    "        metrics = [RMSE(), MAPE(), QError(percentile=50, early_stopping_metric=True), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "    elif loss_class_name == 'MSELoss':\n",
    "        metrics = [RMSE(early_stopping_metric=True), MAPE(), QError(percentile=50), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "\n",
    "    # create zero shot model dependent on database\n",
    "    model = zero_shot_models[database](device=device, hidden_dim=hidden_dim, final_mlp_kwargs=final_mlp_kwargs,\n",
    "                                       node_type_kwargs=node_type_kwargs, output_dim=output_dim,\n",
    "                                       feature_statistics=feature_statistics, tree_layer_name=tree_layer_name,\n",
    "                                       tree_layer_kwargs=tree_layer_kwargs,\n",
    "                                       plan_featurization_name=plan_featurization_name,\n",
    "                                       label_norm=label_norm,\n",
    "                                       **model_kwargs)\n",
    "    # move to gpu\n",
    "    model = model.to(model.device)\n",
    "    optimizer = opt.__dict__[optimizer_class_name](model.parameters(), **optimizer_kwargs)\n",
    "    csv_stats, epochs_wo_improvement, epoch, model, optimizer, metrics, finished = \\\n",
    "        load_checkpoint(model, target_dir, filename_model, optimizer=optimizer, metrics=metrics, filetype='.pt')\n",
    "    return test_loaders, model\n",
    "\n",
    "def load_model(workload_runs,\n",
    "            test_workload_runs,\n",
    "               statistics_file,\n",
    "               target_dir,\n",
    "               filename_model,\n",
    "               hyperparameter_path,\n",
    "              device='cpu',\n",
    "              max_epoch_tuples=100000,\n",
    "              num_workers=1,\n",
    "              loss_class_name='QLoss',\n",
    "              database=None,\n",
    "              seed=0,\n",
    "              limit_queries=None,\n",
    "              limit_queries_affected_wl=None,\n",
    "              max_no_epochs=None,\n",
    "              skip_train=False\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Reads out hyperparameters and trains model\n",
    "    \"\"\"\n",
    "    print(f\"Reading hyperparameters from {hyperparameter_path}\")\n",
    "    hyperparams = load_json(hyperparameter_path, namespace=False)\n",
    "\n",
    "    p_dropout = hyperparams.pop('p_dropout')\n",
    "    # general fc out\n",
    "    fc_out_kwargs = dict(p_dropout=p_dropout,\n",
    "                         activation_class_name='LeakyReLU',\n",
    "                         activation_class_kwargs={},\n",
    "                         norm_class_name='Identity',\n",
    "                         norm_class_kwargs={},\n",
    "                         residual=hyperparams.pop('residual'),\n",
    "                         dropout=hyperparams.pop('dropout'),\n",
    "                         activation=True,\n",
    "                         inplace=True)\n",
    "    final_mlp_kwargs = dict(width_factor=hyperparams.pop('final_width_factor'),\n",
    "                            n_layers=hyperparams.pop('final_layers'),\n",
    "                            loss_class_name=loss_class_name,\n",
    "                            loss_class_kwargs=dict())\n",
    "    tree_layer_kwargs = dict(width_factor=hyperparams.pop('tree_layer_width_factor'),\n",
    "                             n_layers=hyperparams.pop('message_passing_layers'))\n",
    "    node_type_kwargs = dict(width_factor=hyperparams.pop('node_type_width_factor'),\n",
    "                            n_layers=hyperparams.pop('node_layers'),\n",
    "                            one_hot_embeddings=True,\n",
    "                            max_emb_dim=hyperparams.pop('max_emb_dim'),\n",
    "                            drop_whole_embeddings=False)\n",
    "    final_mlp_kwargs.update(**fc_out_kwargs)\n",
    "    tree_layer_kwargs.update(**fc_out_kwargs)\n",
    "    node_type_kwargs.update(**fc_out_kwargs)\n",
    "\n",
    "    train_kwargs = dict(optimizer_class_name='AdamW',\n",
    "                        optimizer_kwargs=dict(\n",
    "                            lr=hyperparams.pop('lr'),\n",
    "                        ),\n",
    "                        final_mlp_kwargs=final_mlp_kwargs,\n",
    "                        node_type_kwargs=node_type_kwargs,\n",
    "                        tree_layer_kwargs=tree_layer_kwargs,\n",
    "                        tree_layer_name=hyperparams.pop('tree_layer_name'),\n",
    "                        plan_featurization_name=hyperparams.pop('plan_featurization_name'),\n",
    "                        hidden_dim=hyperparams.pop('hidden_dim'),\n",
    "                        output_dim=1,\n",
    "                        epochs=200 if max_no_epochs is None else max_no_epochs,\n",
    "                        early_stopping_patience=20,\n",
    "                        max_epoch_tuples=max_epoch_tuples,\n",
    "                        batch_size=hyperparams.pop('batch_size'),\n",
    "                        device=device,\n",
    "                        num_workers=num_workers,\n",
    "                        seed=seed,\n",
    "                        limit_queries=limit_queries,\n",
    "                        limit_queries_affected_wl=limit_queries_affected_wl,\n",
    "                        skip_train=skip_train\n",
    "                        )\n",
    "\n",
    "    assert len(hyperparams) == 0, f\"Not all hyperparams were used (not used: {hyperparams.keys()}). Hence generation \" \\\n",
    "                                  f\"and reading does not seem to fit\"\n",
    "\n",
    "    param_dict = flatten_dict(train_kwargs)\n",
    "    \n",
    "    test_loaders, model = training_model_loader(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                          param_dict=param_dict, database=database, **train_kwargs)\n",
    "    \n",
    "    return test_loaders, model\n",
    "\n",
    "def validate_model(val_loader, model, epoch=0, epoch_stats=None, metrics=None, max_epoch_tuples=None,\n",
    "                   custom_batch_to=batch_to, verbose=False, log_all_queries=False):\n",
    "    model.eval()\n",
    "    print(model.plan_featurization_name)\n",
    "    with torch.autograd.no_grad():\n",
    "        val_loss = torch.Tensor([0])\n",
    "        labels = []\n",
    "        preds = []\n",
    "        probs = []\n",
    "        sample_idxs = []\n",
    "\n",
    "        # evaluate test set using model\n",
    "        test_start_t = time.perf_counter()\n",
    "        val_num_tuples = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader)):\n",
    "            if max_epoch_tuples is not None and batch_idx * val_loader.batch_size > max_epoch_tuples:\n",
    "                break\n",
    "            val_num_tuples += val_loader.batch_size\n",
    "\n",
    "            input_model, label, sample_idxs_batch = custom_batch_to(batch, model.device, model.label_norm)\n",
    "            #sample_idxs += sample_idxs_batch\n",
    "            output = model(input_model)\n",
    "\n",
    "            # sum up mean batch losses\n",
    "            val_loss += model.loss_fxn(output, label).cpu()\n",
    "\n",
    "            # inverse transform the predictions and labels\n",
    "            curr_pred = output.cpu().numpy()\n",
    "            curr_label = label.cpu().numpy()\n",
    "            if model.label_norm is not None:\n",
    "                curr_pred = model.label_norm.inverse_transform(curr_pred)\n",
    "                curr_label = model.label_norm.inverse_transform(curr_label.reshape(-1, 1))\n",
    "                curr_label = curr_label.reshape(-1)\n",
    "           \n",
    "            preds.append(curr_pred.reshape(-1))\n",
    "            labels.append(curr_label.reshape(-1))\n",
    "\n",
    "        if epoch_stats is not None:\n",
    "            epoch_stats.update(val_time=time.perf_counter() - test_start_t)\n",
    "            epoch_stats.update(val_num_tuples=val_num_tuples)\n",
    "            val_loss = (val_loss.cpu() / len(val_loader)).item()\n",
    "            print(f'val_loss epoch {epoch}: {val_loss}')\n",
    "            epoch_stats.update(val_loss=val_loss)\n",
    "\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0332eb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_model(target_dir, filename_model, hyperparameter_path):\n",
    "    database = DatabaseSystem.POSTGRES\n",
    "    workload_runs = [\"/home/ziniuw//zero-shot-data/runs/parsed_plans/imdb_full/complex_workload_400k_s4_c8220.json\"]\n",
    "    #test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/job_full_c8220.json\"]\n",
    "    test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_testing_2k.json\"]\n",
    "    statistics_file = \"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "    test_loaders, model = load_model(test_workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                  hyperparameter_path, database=database)\n",
    "    true, pred = validate_model(test_loaders[0], model)\n",
    "    qerror = np.maximum(true/pred, pred/true)\n",
    "    res = []\n",
    "    for i in [50, 95, 99]:\n",
    "        print(f\"{i} percentile is {np.percentile(qerror, i)}\")\n",
    "        res.append(np.percentile(qerror, i))\n",
    "    return res, true, pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "617dfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\", \"r\") as f:\n",
    "    hyperparameters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2633ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters[\"plan_featurization_name\"] = \"PostgresCardCorrectorDetail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a3920e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_cc_est_best_config.json\", \"w\") as f:\n",
    "    json.dump(hyperparameters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1c12090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************************************\n",
      "evaluting models with PG_est_50000 fine tuning queries with True Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 2 (2 csv rows) in 0.060 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2445787191390991\n",
      "95 percentile is 2.3224584102630614\n",
      "99 percentile is 3.512959938049317\n",
      "***************************************************************\n",
      "evaluting models with PG_est_50000 fine tuning queries with PG_est Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 2 (2 csv rows) in 0.059 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2784109115600586\n",
      "95 percentile is 2.5780063152313226\n",
      "99 percentile is 3.9000282764434826\n",
      "***************************************************************\n",
      "evaluting models with PG_est_20000 fine tuning queries with True Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 98 (98 csv rows) in 0.061 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.1769758462905884\n",
      "95 percentile is 2.466688251495361\n",
      "99 percentile is 4.194423236846925\n",
      "***************************************************************\n",
      "evaluting models with PG_est_20000 fine tuning queries with PG_est Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 98 (98 csv rows) in 0.060 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.1719073057174683\n",
      "95 percentile is 2.4645878791809075\n",
      "99 percentile is 4.081901016235354\n",
      "***************************************************************\n",
      "evaluting models with PG_est_1000 fine tuning queries with True Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 25 (25 csv rows) in 0.062 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.212053894996643\n",
      "95 percentile is 2.532796525955199\n",
      "99 percentile is 3.750665321350098\n",
      "***************************************************************\n",
      "evaluting models with PG_est_1000 fine tuning queries with PG_est Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 25 (25 csv rows) in 0.058 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2764558792114258\n",
      "95 percentile is 2.601836347579955\n",
      "99 percentile is 3.77589454650879\n",
      "***************************************************************\n",
      "evaluting models with True_10000 fine tuning queries with True Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 14 (14 csv rows) in 0.058 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.1795276403427124\n",
      "95 percentile is 2.2349344730377183\n",
      "99 percentile is 3.3265203762054445\n",
      "***************************************************************\n",
      "evaluting models with True_10000 fine tuning queries with PG_est Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 14 (14 csv rows) in 0.062 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:06<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2858800888061523\n",
      "95 percentile is 2.603770399093628\n",
      "99 percentile is 3.643503561019899\n",
      "***************************************************************\n",
      "evaluting models with True_1000 fine tuning queries with True Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 16 (16 csv rows) in 0.059 secs\n",
      "PostgresTrueCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2040833234786987\n",
      "95 percentile is 2.3314047336578363\n",
      "99 percentile is 3.3336418628692637\n",
      "***************************************************************\n",
      "evaluting models with True_1000 fine tuning queries with PG_est Card:\n",
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n",
      "No of Plans: 2029\n",
      "No of Plans: 2029\n",
      "Successfully loaded checkpoint from epoch 16 (16 csv rows) in 0.059 secs\n",
      "PostgresEstSystemCardDetail\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:07<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 percentile is 1.2978789806365967\n",
      "95 percentile is 2.6432095527648922\n",
      "99 percentile is 3.769691743850708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models_files = {\n",
    "    \"PG_est_50000\": \"imdb_full_est_lmtq_50000\",\n",
    "    \"PG_est_20000\": \"imdb_full_est_lmtq_20000\",\n",
    "    \"PG_est_1000\": \"imdb_full_est_lmtq_1000\",\n",
    "    \"True_10000\": \"imdb_full_lmtq_10000\",\n",
    "    \"True_1000\": \"imdb_full_lmtq_1000\",\n",
    "}\n",
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_few_shot_tune\"\n",
    "hyperparameter_paths = {\n",
    "    \"True Card\": \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\",\n",
    "    \"PG_est Card\": \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\"\n",
    "}\n",
    "\n",
    "\n",
    "for num_query in models_files:\n",
    "    filename_model = models_files[num_query]\n",
    "    for test_with in hyperparameter_paths:\n",
    "        hyperparameter_path = hyperparameter_paths[test_with]\n",
    "        print(\"***************************************************************\")\n",
    "        print(f\"evaluting models with {num_query} fine tuning queries with {test_with}:\")\n",
    "        res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_few_shot_tune\"\n",
    "filename_model = \"imdb_full_lmtq_1000\"\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\"\n",
    "res, true, pred = test_one_model(target_dir, filename_model, hyperparameter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c901f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = DatabaseSystem.POSTGRES\n",
    "workload_runs = [\"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_workload_400k_s4_c8220.json\",\n",
    "                 \"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_workload_400k_s5_c8220.json\", \n",
    "                 \"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_workload_400k_s6_c8220.json\"]\n",
    "test_workload_runs = [\"/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/job_full_c8220.json\"]\n",
    "statistics_file = \"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "target_dir = \"/home/ziniuw/zero-shot-data/evaluation/job_full_tune/\"\n",
    "filename_model = \"imdb_full_0\"\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_best_config.json\"\n",
    "limit_queries = 1000\n",
    "ft_filename_model = filename_model + f\"_ft_{limit_queries}\"\n",
    "\n",
    "test_loaders, model = load_model(test_workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                  hyperparameter_path, database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, pred = validate_model(test_loaders[0], model)\n",
    "qerror = np.maximum(true/pred, pred/true)\n",
    "print(\"performance without finetuning:\")\n",
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(qerror, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9385ef11",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(true)\n",
    "print(qerror[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4edd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, pred = validate_model(test_loaders[0], model)\n",
    "qerror = np.maximum(true/pred, pred/true)\n",
    "print(\"performance without finetuning:\")\n",
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(qerror, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218f7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_readout_hyperparams(workload_runs, test_workload_runs, statistics_file, target_dir, ft_filename_model,\n",
    "                          hyperparameter_path, num_workers=16, database=database, max_no_epochs=50, \n",
    "                          limit_queries=limit_queries, limit_queries_affected_wl=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf950c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, pred = validate_model(test_loaders[0], model)\n",
    "qerror = np.maximum(true/pred, pred/true)\n",
    "print(f\"performance with {limit_queries} finetuning:\")\n",
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(qerror, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b1a8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 1000, 5000, 10000, 50000]\n",
    "true_true = [1.26, 1.26, 1.26, 1.26, 1.26]\n",
    "true_true_ft = [1.26, 1.27, 1.25, 1.27, 1.24]\n",
    "true_est = [1.61, 1.61, 1.61, 1.61, 1.61]\n",
    "true_est_ft = [1.61, 1.57, 1.62, 1.58, 1.63]\n",
    "est_true = [1.46, 1.46, 1.46, 1.46, 1.46]\n",
    "est_true_ft = [1.46, 1.47, 1.35, 1.35, 1.41]\n",
    "est_est = [1.55, 1.55, 1.55, 1.55, 1.55]\n",
    "est_est_ft = [1.55, 1.48, 1.36, 1.31, 1.29]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, true_true, color=\"red\")\n",
    "plt.plot(x, true_true_ft, color=\"red\", linestyle='dashed')\n",
    "plt.scatter(x, true_true_ft, color=\"red\", s=50)\n",
    "\n",
    "plt.plot(x, true_est, color=\"orange\")\n",
    "plt.plot(x, true_est_ft, color=\"orange\", linestyle='dashed')\n",
    "plt.scatter(x, true_est_ft, color=\"orange\", s=50)\n",
    "\n",
    "plt.plot(x, est_true, color=\"blue\")\n",
    "plt.plot(x, est_true_ft, color=\"blue\", linestyle='dashed')\n",
    "plt.scatter(x, est_true_ft, color=\"blue\", s=50)\n",
    "\n",
    "plt.plot(x, est_est, color=\"black\")\n",
    "plt.plot(x, est_est_ft, color=\"black\", linestyle='dashed')\n",
    "plt.scatter(x, est_est_ft, color=\"black\", s=50)\n",
    "\n",
    "plt.ylim(1.2, 1.7)\n",
    "plt.ylabel(\"Median Qerror\", fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Training Queries\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.savefig(\"jobfull.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e79752",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [10, 1000, 5000, 10000, 50000]\n",
    "true_true = [1.26, 1.26, 1.26, 1.26, 1.26]\n",
    "true_true_ft = [1.26, 1.27, 1.25, 1.27, 1.24]\n",
    "true_est = [1.61, 1.61, 1.61, 1.61, 1.61]\n",
    "true_est_ft = [1.61, 1.57, 1.62, 1.58, 1.63]\n",
    "est_true = [1.46, 1.46, 1.46, 1.46, 1.46]\n",
    "est_true_ft = [1.46, 1.47, 1.35, 1.35, 1.41]\n",
    "est_est = [1.55, 1.55, 1.55, 1.55, 1.55]\n",
    "est_est_ft = [1.55, 1.48, 1.36, 1.31, 1.29]\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(x, true_true, color=\"red\")\n",
    "plt.plot(x, true_true_ft, color=\"red\", linestyle='dashed')\n",
    "plt.scatter(x, true_true_ft, color=\"red\", s=50)\n",
    "\n",
    "#plt.plot(x, true_est, color=\"orange\")\n",
    "#plt.plot(x, true_est_ft, color=\"orange\", linestyle='dashed')\n",
    "#plt.scatter(x, true_est_ft, color=\"orange\", s=50)\n",
    "\n",
    "#plt.plot(x, est_true, color=\"blue\")\n",
    "#plt.plot(x, est_true_ft, color=\"blue\", linestyle='dashed')\n",
    "#plt.scatter(x, est_true_ft, color=\"blue\", s=50)\n",
    "\n",
    "plt.plot(x, est_est, color=\"black\")\n",
    "plt.plot(x, est_est_ft, color=\"black\", linestyle='dashed')\n",
    "plt.scatter(x, est_est_ft, color=\"black\", s=50)\n",
    "\n",
    "plt.ylim(1.2, 1.7)\n",
    "plt.ylabel(\"Median Qerror\", fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Training Queries\", fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.grid()\n",
    "plt.savefig(\"jobfull.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e313b62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
