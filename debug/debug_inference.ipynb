{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fddb3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/home/ziniuw/zero-shot-cost-estimation\")\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "from cross_db_benchmark.benchmark_tools.database import DatabaseSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ad78c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import torch\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from models.dataset.dataset_creation import create_dataloader\n",
    "from models.training.checkpoint import save_checkpoint, load_checkpoint, save_csv\n",
    "from models.training.metrics import MAPE, RMSE, QError\n",
    "from models.training.utils import batch_to, flatten_dict, find_early_stopping_metric\n",
    "from models.zero_shot_models.specific_models.model import zero_shot_models\n",
    "\n",
    "def training_model_loader(workload_runs,\n",
    "                test_workload_runs,\n",
    "                statistics_file,\n",
    "                target_dir,\n",
    "                filename_model,\n",
    "                optimizer_class_name='Adam',\n",
    "                optimizer_kwargs=None,\n",
    "                final_mlp_kwargs=None,\n",
    "                node_type_kwargs=None,\n",
    "                model_kwargs=None,\n",
    "                tree_layer_name='GATConv',\n",
    "                tree_layer_kwargs=None,\n",
    "                hidden_dim=32,\n",
    "                batch_size=32,\n",
    "                output_dim=1,\n",
    "                epochs=0,\n",
    "                device='cpu',\n",
    "                plan_featurization_name=None,\n",
    "                max_epoch_tuples=100000,\n",
    "                param_dict=None,\n",
    "                num_workers=1,\n",
    "                early_stopping_patience=20,\n",
    "                trial=None,\n",
    "                database=None,\n",
    "                limit_queries=None,\n",
    "                limit_queries_affected_wl=None,\n",
    "                skip_train=False,\n",
    "                seed=0):\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = dict()\n",
    "\n",
    "    # seed for reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    target_test_csv_paths = []\n",
    "    if test_workload_runs is not None:\n",
    "        for p in test_workload_runs:\n",
    "            test_workload = os.path.basename(p).replace('.json', '')\n",
    "            target_test_csv_paths.append(os.path.join(target_dir, f'test_{filename_model}_{test_workload}.csv'))\n",
    "\n",
    "    if len(target_test_csv_paths) > 0 and all([os.path.exists(p) for p in target_test_csv_paths]):\n",
    "        print(f\"Model was already trained and tested ({target_test_csv_paths} exists)\")\n",
    "        return\n",
    "\n",
    "    # create a dataset\n",
    "    loss_class_name = final_mlp_kwargs['loss_class_name']\n",
    "    label_norm, feature_statistics, train_loader, val_loader, test_loaders = \\\n",
    "        create_dataloader(workload_runs, test_workload_runs, statistics_file, plan_featurization_name, database,\n",
    "                          val_ratio=0.15, batch_size=batch_size, shuffle=True, num_workers=num_workers,\n",
    "                          pin_memory=False, limit_queries=limit_queries,\n",
    "                          limit_queries_affected_wl=limit_queries_affected_wl, loss_class_name=loss_class_name)\n",
    "    print(len(train_loader))\n",
    "\n",
    "    if loss_class_name == 'QLoss':\n",
    "        metrics = [RMSE(), MAPE(), QError(percentile=50, early_stopping_metric=True), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "    elif loss_class_name == 'MSELoss':\n",
    "        metrics = [RMSE(early_stopping_metric=True), MAPE(), QError(percentile=50), QError(percentile=95),\n",
    "                   QError(percentile=100)]\n",
    "\n",
    "    # create zero shot model dependent on database\n",
    "    model = zero_shot_models[database](device=device, hidden_dim=hidden_dim, final_mlp_kwargs=final_mlp_kwargs,\n",
    "                                       node_type_kwargs=node_type_kwargs, output_dim=output_dim,\n",
    "                                       feature_statistics=feature_statistics, tree_layer_name=tree_layer_name,\n",
    "                                       tree_layer_kwargs=tree_layer_kwargs,\n",
    "                                       plan_featurization_name=plan_featurization_name,\n",
    "                                       label_norm=label_norm,\n",
    "                                       **model_kwargs)\n",
    "    # move to gpu\n",
    "    model = model.to(model.device)\n",
    "    print(model)\n",
    "    optimizer = opt.__dict__[optimizer_class_name](model.parameters(), **optimizer_kwargs)\n",
    "    csv_stats, epochs_wo_improvement, epoch, model, optimizer, metrics, finished = \\\n",
    "        load_checkpoint(model, target_dir, filename_model, optimizer=optimizer, metrics=metrics, filetype='.pt')\n",
    "    return test_loaders, model\n",
    "\n",
    "def load_model(workload_runs,\n",
    "            test_workload_runs,\n",
    "               statistics_file,\n",
    "               target_dir,\n",
    "               filename_model,\n",
    "               hyperparameter_path,\n",
    "              device='cpu',\n",
    "              max_epoch_tuples=100000,\n",
    "              num_workers=1,\n",
    "              loss_class_name='QLoss',\n",
    "              database=None,\n",
    "              seed=0,\n",
    "              limit_queries=None,\n",
    "              limit_queries_affected_wl=None,\n",
    "              max_no_epochs=None,\n",
    "              skip_train=False\n",
    "              ):\n",
    "    \"\"\"\n",
    "    Reads out hyperparameters and trains model\n",
    "    \"\"\"\n",
    "    print(f\"Reading hyperparameters from {hyperparameter_path}\")\n",
    "    hyperparams = load_json(hyperparameter_path, namespace=False)\n",
    "\n",
    "    p_dropout = hyperparams.pop('p_dropout')\n",
    "    # general fc out\n",
    "    fc_out_kwargs = dict(p_dropout=p_dropout,\n",
    "                         activation_class_name='LeakyReLU',\n",
    "                         activation_class_kwargs={},\n",
    "                         norm_class_name='Identity',\n",
    "                         norm_class_kwargs={},\n",
    "                         residual=hyperparams.pop('residual'),\n",
    "                         dropout=hyperparams.pop('dropout'),\n",
    "                         activation=True,\n",
    "                         inplace=True)\n",
    "    final_mlp_kwargs = dict(width_factor=hyperparams.pop('final_width_factor'),\n",
    "                            n_layers=hyperparams.pop('final_layers'),\n",
    "                            loss_class_name=loss_class_name,\n",
    "                            loss_class_kwargs=dict())\n",
    "    tree_layer_kwargs = dict(width_factor=hyperparams.pop('tree_layer_width_factor'),\n",
    "                             n_layers=hyperparams.pop('message_passing_layers'))\n",
    "    node_type_kwargs = dict(width_factor=hyperparams.pop('node_type_width_factor'),\n",
    "                            n_layers=hyperparams.pop('node_layers'),\n",
    "                            one_hot_embeddings=True,\n",
    "                            max_emb_dim=hyperparams.pop('max_emb_dim'),\n",
    "                            drop_whole_embeddings=False)\n",
    "    final_mlp_kwargs.update(**fc_out_kwargs)\n",
    "    tree_layer_kwargs.update(**fc_out_kwargs)\n",
    "    node_type_kwargs.update(**fc_out_kwargs)\n",
    "\n",
    "    train_kwargs = dict(optimizer_class_name='AdamW',\n",
    "                        optimizer_kwargs=dict(\n",
    "                            lr=hyperparams.pop('lr'),\n",
    "                        ),\n",
    "                        final_mlp_kwargs=final_mlp_kwargs,\n",
    "                        node_type_kwargs=node_type_kwargs,\n",
    "                        tree_layer_kwargs=tree_layer_kwargs,\n",
    "                        tree_layer_name=hyperparams.pop('tree_layer_name'),\n",
    "                        plan_featurization_name=hyperparams.pop('plan_featurization_name'),\n",
    "                        hidden_dim=hyperparams.pop('hidden_dim'),\n",
    "                        output_dim=1,\n",
    "                        epochs=200 if max_no_epochs is None else max_no_epochs,\n",
    "                        early_stopping_patience=20,\n",
    "                        max_epoch_tuples=max_epoch_tuples,\n",
    "                        batch_size=hyperparams.pop('batch_size'),\n",
    "                        device=device,\n",
    "                        num_workers=num_workers,\n",
    "                        seed=seed,\n",
    "                        limit_queries=limit_queries,\n",
    "                        limit_queries_affected_wl=limit_queries_affected_wl,\n",
    "                        skip_train=skip_train\n",
    "                        )\n",
    "\n",
    "    assert len(hyperparams) == 0, f\"Not all hyperparams were used (not used: {hyperparams.keys()}). Hence generation \" \\\n",
    "                                  f\"and reading does not seem to fit\"\n",
    "\n",
    "    param_dict = flatten_dict(train_kwargs)\n",
    "    \n",
    "    test_loaders, model = training_model_loader(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                          param_dict=param_dict, database=database, **train_kwargs)\n",
    "    \n",
    "    return test_loaders, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00bbe0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(val_loader, model, epoch=0, epoch_stats=None, metrics=None, max_epoch_tuples=None,\n",
    "                   custom_batch_to=batch_to, verbose=False, log_all_queries=False):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.autograd.no_grad():\n",
    "        val_loss = torch.Tensor([0])\n",
    "        labels = []\n",
    "        preds = []\n",
    "        probs = []\n",
    "        sample_idxs = []\n",
    "\n",
    "        # evaluate test set using model\n",
    "        test_start_t = time.perf_counter()\n",
    "        val_num_tuples = 0\n",
    "        for batch_idx, batch in enumerate(tqdm(val_loader)):\n",
    "            if max_epoch_tuples is not None and batch_idx * val_loader.batch_size > max_epoch_tuples:\n",
    "                break\n",
    "\n",
    "            val_num_tuples += val_loader.batch_size\n",
    "\n",
    "            input_model, label, sample_idxs_batch = custom_batch_to(batch, model.device, model.label_norm)\n",
    "            sample_idxs += sample_idxs_batch\n",
    "            output = model(input_model)\n",
    "\n",
    "            # sum up mean batch losses\n",
    "            val_loss += model.loss_fxn(output, label).cpu()\n",
    "\n",
    "            # inverse transform the predictions and labels\n",
    "            curr_pred = output.cpu().numpy()\n",
    "            curr_label = label.cpu().numpy()\n",
    "            if model.label_norm is not None:\n",
    "                curr_pred = model.label_norm.inverse_transform(curr_pred)\n",
    "                curr_label = model.label_norm.inverse_transform(curr_label.reshape(-1, 1))\n",
    "                curr_label = curr_label.reshape(-1)\n",
    "           \n",
    "            preds.append(curr_pred.reshape(-1))\n",
    "            labels.append(curr_label.reshape(-1))\n",
    "\n",
    "        if epoch_stats is not None:\n",
    "            epoch_stats.update(val_time=time.perf_counter() - test_start_t)\n",
    "            epoch_stats.update(val_num_tuples=val_num_tuples)\n",
    "            val_loss = (val_loss.cpu() / len(val_loader)).item()\n",
    "            print(f'val_loss epoch {epoch}: {val_loss}')\n",
    "            epoch_stats.update(val_loss=val_loss)\n",
    "\n",
    "        labels = np.concatenate(labels, axis=0)\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        return labels, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fefee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hyperparameters from /home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../zero-shot-data/runs/parsed_plans/airline/complex_workload_200k_s1_c8220.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m filename_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb_full_0_pg_est\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m hyperparameter_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m test_loaders, model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_workload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatistics_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mhyperparameter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, hyperparameter_path, device, max_epoch_tuples, num_workers, loss_class_name, database, seed, limit_queries, limit_queries_affected_wl, max_no_epochs, skip_train)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hyperparams) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot all hyperparams were used (not used: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhyperparams\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Hence generation \u001b[39m\u001b[38;5;124m\"\u001b[39m \\\n\u001b[1;32m    166\u001b[0m                               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand reading does not seem to fit\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m flatten_dict(train_kwargs)\n\u001b[0;32m--> 170\u001b[0m test_loaders, model \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_model_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_workload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatistics_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_loaders, model\n",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36mtraining_model_loader\u001b[0;34m(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, optimizer_class_name, optimizer_kwargs, final_mlp_kwargs, node_type_kwargs, model_kwargs, tree_layer_name, tree_layer_kwargs, hidden_dim, batch_size, output_dim, epochs, device, plan_featurization_name, max_epoch_tuples, param_dict, num_workers, early_stopping_patience, trial, database, limit_queries, limit_queries_affected_wl, skip_train, seed)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# create a dataset\u001b[39;00m\n\u001b[1;32m     62\u001b[0m loss_class_name \u001b[38;5;241m=\u001b[39m final_mlp_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_class_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     63\u001b[0m label_norm, feature_statistics, train_loader, val_loader, test_loaders \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_workload_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatistics_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan_featurization_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_class_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_class_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_class_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQLoss\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/zero-shot-cost-estimation/models/dataset/dataset_creation.py:113\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(workload_run_paths, test_workload_run_paths, statistics_file, plan_featurization_name, database, val_ratio, batch_size, shuffle, num_workers, pin_memory, limit_queries, limit_queries_affected_wl, loss_class_name)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03mCreates dataloaders that batches physical plans to train the model in a distributed fashion.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m:param workload_run_paths:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# split plans into train/test/validation\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m label_norm, train_dataset, val_dataset, database_statistics \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkload_run_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mloss_class_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_class_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mval_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mlimit_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m                                                                              \u001b[49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# postgres_plan_collator does the heavy lifting of creating the graphs and extracting the features and thus requires both\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# database statistics but also feature statistics\u001b[39;00m\n\u001b[1;32m    121\u001b[0m feature_statistics \u001b[38;5;241m=\u001b[39m load_json(statistics_file, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/zero-shot-cost-estimation/models/dataset/dataset_creation.py:51\u001b[0m, in \u001b[0;36mcreate_datasets\u001b[0;34m(workload_run_paths, cap_training_samples, val_ratio, limit_queries, limit_queries_affected_wl, shuffle_before_split, loss_class_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_datasets\u001b[39m(workload_run_paths, cap_training_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, val_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.15\u001b[39m, limit_queries\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m                     limit_queries_affected_wl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shuffle_before_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, loss_class_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 51\u001b[0m     plans, database_statistics \u001b[38;5;241m=\u001b[39m \u001b[43mread_workload_runs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworkload_run_paths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_queries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit_queries_affected_wl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     no_plans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(plans)\n\u001b[1;32m     55\u001b[0m     plan_idxs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(no_plans))\n",
      "File \u001b[0;32m~/zero-shot-cost-estimation/models/dataset/dataset_creation.py:21\u001b[0m, in \u001b[0;36mread_workload_runs\u001b[0;34m(workload_run_paths, limit_queries, limit_queries_affected_wl)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, source \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(workload_run_paths):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m         run \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError:\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/zero-shot-cost-estimation/cross_db_benchmark/benchmark_tools/utils.py:25\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(path, namespace)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(path, namespace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m namespace:\n\u001b[1;32m     27\u001b[0m             json_obj \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m d: SimpleNamespace(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../zero-shot-data/runs/parsed_plans/airline/complex_workload_200k_s1_c8220.json'"
     ]
    }
   ],
   "source": [
    "database = DatabaseSystem.POSTGRES\n",
    "workload_runs = [\"../zero-shot-data/runs/parsed_plans/airline/complex_workload_200k_s1_c8220.json\"]\n",
    "test_workload_runs = [\"../zero-shot-data/runs/parsed_plans/imdb_full/job_full_c8220.json\"]\n",
    "statistics_file = \"../zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "target_dir = \"../zero-shot-data/evaluation/job_full_tune/\"\n",
    "filename_model = \"imdb_full_0_pg_est\"\n",
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_est_best_config.json\"\n",
    "\n",
    "test_loaders, model = load_model(workload_runs, test_workload_runs, statistics_file, target_dir, filename_model, \n",
    "                  hyperparameter_path, database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8f6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "true, pred = validate_model(test_loaders[0], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ece5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(true), np.max(true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a0e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b09170",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.maximum(true/pred, pred/true)\n",
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(qerror, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb5dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.maximum(true/pred, pred/true)\n",
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(np.percentile(qerror, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d2fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99530fb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
