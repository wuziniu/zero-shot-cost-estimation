{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25705b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sys.path.append(\"/home/ziniuw/zero-shot-cost-estimation\")\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from data_driven_cardinalities.deepdb.schemas.generate_schema import gen_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ebd67dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598\n"
     ]
    }
   ],
   "source": [
    "result_path = \"/flash1/ziniuw/CEB/zero-shot-results/MSCN3751517648/queries/zero-shot-test2-preds/\"\n",
    "all_MSCN_est = dict()\n",
    "for file in os.listdir(result_path):\n",
    "    if file.endswith(\".pkl\"):\n",
    "        query_no = int(file.split(\".pkl\")[0])\n",
    "        with open(result_path + file, \"rb\") as f:\n",
    "            all_MSCN_est[query_no] = pickle.load(f)\n",
    "print(len(all_MSCN_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59af4511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029\n"
     ]
    }
   ],
   "source": [
    "query_sql_file = \"/home/ziniuw/zero-shot-data/runs/raw/imdb_full/complex_queries_testing_2k.json\"\n",
    "with open(query_sql_file, \"r\") as f:\n",
    "    query_sql = json.load(f)\n",
    "print(len(query_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21f19448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2029\n"
     ]
    }
   ],
   "source": [
    "query_plan_file = '/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_testing_2k.json'\n",
    "queries = load_json(query_plan_file, namespace=True)\n",
    "print(len(queries.parsed_plans))\n",
    "schema = gen_schema(\"imdb_full\", \"/home/ziniuw/zero-shot-data/datasets/imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80676e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import types\n",
    "import os\n",
    "from json import JSONDecodeError\n",
    "from time import perf_counter\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cross_db_benchmark.benchmark_tools.parse_run import dumper\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from models.training.checkpoint import save_csv\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_table_aliases_imdb():\n",
    "    table_aliases = dict()\n",
    "    table_aliases[\"title\"] = \"t\"\n",
    "    table_aliases[\"cast_info\"] = \"ci\"\n",
    "    table_aliases[\"movie_info\"] = \"mi\"\n",
    "    table_aliases[\"movie_info_idx\"] = \"mii\"\n",
    "    table_aliases[\"person_info\"] = \"pi\"\n",
    "    table_aliases[\"name\"] = \"n\"\n",
    "    table_aliases[\"aka_name\"] = \"an\"\n",
    "    table_aliases[\"keyword\"] = \"k\"\n",
    "    table_aliases[\"movie_keyword\"] = \"mk\"\n",
    "    table_aliases[\"movie_companies\"] = \"mc\"\n",
    "    table_aliases[\"movie_link\"] = \"ml\"\n",
    "    table_aliases[\"aka_title\"] = \"at\"\n",
    "    table_aliases[\"complete_cast\"] = \"cc\"\n",
    "    table_aliases[\"kind_type\"] = \"kt\"\n",
    "    table_aliases[\"role_type\"] = \"rt\"\n",
    "    table_aliases[\"char_name\"] = \"chn\"\n",
    "    table_aliases[\"info_type\"] = \"it\"\n",
    "    table_aliases[\"company_type\"] = \"ct\"\n",
    "    table_aliases[\"company_name\"] = \"cn\"\n",
    "    table_aliases[\"movie_link\"] = \"ml\"\n",
    "    table_aliases[\"link_type\"] = \"lt\"\n",
    "    table_aliases[\"comp_cast_type\"] = \"cct\"\n",
    "    return table_aliases\n",
    "\n",
    "\n",
    "def augment_cardinalities(schema, all_MSCN_est, src, table_aliases, target, statistics_file, target_statistics_file,\n",
    "                          scale=1):\n",
    "    try:\n",
    "        run = load_json(src, namespace=True)\n",
    "    except JSONDecodeError:\n",
    "        raise ValueError(f\"Error reading {src}\")\n",
    "\n",
    "    q_stats = []\n",
    "\n",
    "    # find out if this an non_inclusive workload (< previously replaced by <=)\n",
    "    non_inclusive = False\n",
    "    if any([b in src for b in ['job-light', 'scale', 'synthetic']]):\n",
    "        non_inclusive = True\n",
    "        print(\"Assuming NON-INCLUSIVE workload\")\n",
    "\n",
    "    est_pg = 0\n",
    "    est_mscn = 0\n",
    "    all_query_tables = []\n",
    "    for q_id, p in enumerate(tqdm(run.parsed_plans)):\n",
    "        if q_id not in all_MSCN_est:\n",
    "            all_query_tables.append([])\n",
    "            continue\n",
    "        MSCN_est = all_MSCN_est[q_id]\n",
    "        p.plan_parameters.est_pg = 0\n",
    "        p.plan_parameters.est_mscn = 0\n",
    "        all_tables = []\n",
    "        _ = augment_bottom_up(schema, p, q_id, run.database_stats, MSCN_est, table_aliases, q_stats, p, scale,\n",
    "                              non_inclusive=non_inclusive, all_tables=all_tables)\n",
    "        all_query_tables.append(all_tables)\n",
    "        est_pg += p.plan_parameters.est_pg\n",
    "        est_mscn += p.plan_parameters.est_mscn\n",
    "\n",
    "        def augment_prod(p):\n",
    "            if len(p.children) == 0:\n",
    "                p.plan_parameters.cc_est_children_card = 1\n",
    "            else:\n",
    "                child_card = 1\n",
    "                for c in p.children:\n",
    "                    child_card *= c.plan_parameters.cc_est_card\n",
    "                    augment_prod(c)\n",
    "                p.plan_parameters.cc_est_children_card = child_card\n",
    "\n",
    "        augment_prod(p)\n",
    "\n",
    "    argumented_queries = types.SimpleNamespace()\n",
    "    argumented_queries.database_stats = run.database_stats\n",
    "    argumented_queries.run_kwargs = run.run_kwargs\n",
    "    argumented_queries.parsed_plans = []\n",
    "    for q_id, p in enumerate(run.parsed_plans):\n",
    "        if q_id in all_MSCN_est:\n",
    "            argumented_queries.parsed_plans.append(p)\n",
    "\n",
    "    print(len(argumented_queries.parsed_plans))\n",
    "    target_dir = os.path.dirname(target)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with open(target, 'w') as outfile:\n",
    "        json.dump(argumented_queries, outfile, default=dumper)\n",
    "\n",
    "    feature_statistics = load_json(statistics_file, namespace=False)\n",
    "    feature_statistics['est_mscn'] = {'max': 0.0, 'scale': 1.0, 'center': 1.0, 'type': 'numeric'}\n",
    "    feature_statistics['cc_est_card'] = feature_statistics['act_card']\n",
    "    feature_statistics['cc_est_children_card'] = feature_statistics['act_children_card']\n",
    "    with open(target_statistics_file, \"w\") as f:\n",
    "        json.dump(feature_statistics, f)\n",
    "\n",
    "    return all_query_tables, est_mscn, est_pg, q_stats\n",
    "\n",
    "\n",
    "def report_stats(est_mscn, est_pg, q_stats):\n",
    "    if len(q_stats) > 0:\n",
    "        def report_percentiles(key):\n",
    "            vals = np.array([q_s[key] for q_s in q_stats])\n",
    "            print(f\"{key}: p50={np.median(vals):.2f} p95={np.percentile(vals, 95):.2f} \"\n",
    "                  f\"p99={np.percentile(vals, 99):.2f} pmax={np.max(vals):.2f}\")\n",
    "\n",
    "        report_percentiles('q_errors_pg')\n",
    "        report_percentiles('q_errors_mscn')\n",
    "        print(f\"{est_mscn / (est_mscn + est_pg) * 100:.2f}% estimated using MSCN\")\n",
    "\n",
    "\n",
    "def match_sub_queries(tables, MSCN_est, table_aliases, q_id):\n",
    "    aliased_tables = set()\n",
    "    for table in tables:\n",
    "        alias = table_aliases[table]\n",
    "        aliased_tables.add(alias)\n",
    "    for alias in MSCN_est:\n",
    "        alias_set = set(alias)\n",
    "        if alias_set == aliased_tables:\n",
    "            return MSCN_est[alias]\n",
    "    #print(f\"query {q_id}: {aliased_tables} not found in {MSCN_est.keys()}. Replacing with PG estimates\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def augment_bottom_up(schema, plan, q_id, database_statistics, MSCN_est, table_aliases,\n",
    "                      q_stats, top_p, scale, all_tables, non_inclusive=False):\n",
    "    workers_planned = vars(plan.plan_parameters).get('workers_planned')\n",
    "    if workers_planned is None:\n",
    "        workers_planned = 0\n",
    "    # assert workers_planned is not None\n",
    "\n",
    "    aggregation_below = 'Aggregate' in plan.plan_parameters.op_name\n",
    "\n",
    "    # augment own tables\n",
    "    tables = set()\n",
    "    t_idx = vars(plan.plan_parameters).get('table')\n",
    "    if t_idx is not None:\n",
    "        table_stats = database_statistics.table_stats[t_idx]\n",
    "        if hasattr(table_stats, 'relname'):\n",
    "            table_name = table_stats.relname\n",
    "        elif hasattr(table_stats, 'table'):\n",
    "            table_name = table_stats.table\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        tables.add(table_name)\n",
    "\n",
    "    for c in plan.children:\n",
    "        c_aggregation_below, c_tables = augment_bottom_up(schema, c, q_id, database_statistics,\n",
    "                                                          MSCN_est, table_aliases, q_stats,\n",
    "                                                          top_p, scale,\n",
    "                                                          non_inclusive=non_inclusive,\n",
    "                                                          all_tables=all_tables\n",
    "                                                          )\n",
    "        aggregation_below |= c_aggregation_below\n",
    "        tables.update(c_tables)\n",
    "\n",
    "    # evaluate query\n",
    "    act_card, pg_est_card = get_act_est_card(plan.plan_parameters)\n",
    "\n",
    "    query_parsed = True\n",
    "    q = None\n",
    "    if len(tables) == 0:\n",
    "        print(\"Could not parse query\")\n",
    "        query_parsed = False\n",
    "\n",
    "    # query not supported\n",
    "    if not query_parsed:\n",
    "        plan.plan_parameters.cc_est_card = pg_est_card\n",
    "        top_p.plan_parameters.est_pg += 1\n",
    "\n",
    "    # group by not directly supported\n",
    "    elif aggregation_below:\n",
    "        plan.plan_parameters.cc_est_card = pg_est_card\n",
    "        top_p.plan_parameters.est_pg += 1\n",
    "\n",
    "    # we do not care about really small cardinalities\n",
    "    elif (act_card is not None and pg_est_card <= 10 and act_card <= 10):\n",
    "        plan.plan_parameters.cc_est_card = pg_est_card\n",
    "        top_p.plan_parameters.est_pg += 1\n",
    "\n",
    "    else:\n",
    "        if plan.plan_parameters.op_name in {'Parallel Seq Scan', 'Hash Join', 'Nested Loop', 'Seq Scan', 'Materialize',\n",
    "                                            'Hash', 'Parallel Hash', 'Merge Join', 'Gather', 'Gather Merge',\n",
    "                                            'Hash Right Join', 'Hash Left Join', 'Nested Loop Left Join',\n",
    "                                            'Merge Left Join', 'Merge Right Join', 'Index Only Scan', 'Index Scan', 'Parallel Index Only Scan',\n",
    "                                              'Bitmap Index Scan', 'Parallel Bitmap Heap Scan', 'Bitmap Heap Scan',\n",
    "                                              'Sort', 'Parallel Index Scan', 'BitmapAnd'} \\\n",
    "                or plan.plan_parameters.op_name.startswith('XN ') \\\n",
    "                or plan.plan_parameters.op_name in {'Broadcast', 'Distribute'}:\n",
    "            op_name = plan.plan_parameters.op_name\n",
    "\n",
    "            cardinality_predict = match_sub_queries(tables, MSCN_est, table_aliases, q_id)\n",
    "            if cardinality_predict is None:\n",
    "                cardinality_predict = pg_est_card\n",
    "            all_tables.append(copy.deepcopy(tables))\n",
    "            if workers_planned > 0 and (op_name.startswith('Parallel')):\n",
    "                cardinality_predict /= (workers_planned + 1)\n",
    "\n",
    "            if act_card is not None:\n",
    "                q_err_mscn = q_err(cardinality_predict, act_card)\n",
    "                q_err_pg = q_err(pg_est_card, act_card)\n",
    "            else:\n",
    "                q_err_mscn = 1\n",
    "                q_err_pg = 1\n",
    "\n",
    "            # this was probably a bug, anyway rarely happens\n",
    "            if q_err_mscn > 100 * q_err_pg:\n",
    "                plan.plan_parameters.cc_est_card = pg_est_card\n",
    "                top_p.plan_parameters.est_pg += 1\n",
    "            else:\n",
    "                plan.plan_parameters.cc_est_card = cardinality_predict\n",
    "                top_p.plan_parameters.est_mscn += 1\n",
    "\n",
    "                q_stats.append({\n",
    "                    'query_id': q_id,\n",
    "                    'q_errors_pg': q_err_pg,\n",
    "                    'q_errors_mscn': q_err_mscn\n",
    "                })\n",
    "\n",
    "        # ignore this in the stats since pg semantics for cardinalities are different for this operator\n",
    "        elif plan.plan_parameters.op_name in {'Index Only Scan', 'Index Scan', 'Parallel Index Only Scan',\n",
    "                                              'Bitmap Index Scan', 'Parallel Bitmap Heap Scan', 'Bitmap Heap Scan',\n",
    "                                              'Sort', 'Parallel Index Scan', 'BitmapAnd'}:\n",
    "            plan.plan_parameters.cc_est_card = pg_est_card\n",
    "            top_p.plan_parameters.est_pg += 1\n",
    "        else:\n",
    "            raise NotImplementedError(plan.plan_parameters.op_name)\n",
    "\n",
    "    return aggregation_below, tables\n",
    "\n",
    "\n",
    "def get_act_est_card(params):\n",
    "    if hasattr(params, 'act_card'):\n",
    "        act_card = params.act_card\n",
    "        pg_est_card = params.est_card\n",
    "    elif hasattr(params, 'est_rows'):\n",
    "        act_card = params.act_avg_rows\n",
    "        pg_est_card = params.est_rows\n",
    "    # only estimated available\n",
    "    elif hasattr(params, 'est_card'):\n",
    "        # pretend that postgres is true\n",
    "        act_card = None\n",
    "        pg_est_card = params.est_card\n",
    "    else:\n",
    "        print(params)\n",
    "        raise NotImplementedError\n",
    "    return act_card, pg_est_card\n",
    "\n",
    "\n",
    "def q_err(cardinality_predict, cardinality_true):\n",
    "    if cardinality_predict == 0 and cardinality_true == 0:\n",
    "        q_error = 1.\n",
    "    elif cardinality_true == 0:\n",
    "        q_error = 1.\n",
    "    elif cardinality_predict == 0:\n",
    "        q_error = cardinality_true\n",
    "    else:\n",
    "        q_error = max(cardinality_predict / cardinality_true, cardinality_true / cardinality_predict)\n",
    "    return q_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ec09f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_aliases = get_table_aliases_imdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9ecaafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2029/2029 [00:00<00:00, 8921.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598\n",
      "q_errors_pg: p50=1.25 p95=27.51 p99=177.91 pmax=533545.23\n",
      "q_errors_mscn: p50=1.25 p95=10.96 p99=92.36 pmax=59107.28\n",
      "65.05% estimated using MSCN\n",
      "10890\n"
     ]
    }
   ],
   "source": [
    "query_plan_file = '/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_testing_2k.json'\n",
    "target = '/home/ziniuw/zero-shot-data/runs/MSCN_augmented/imdb_full/complex_queries_testing_2k.json'\n",
    "statistics_file = \"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "target_statistics_file = '/home/ziniuw/zero-shot-data/runs/MSCN_augmented/statistics_workload_combined.json'\n",
    "all_tables, est_mscn, est_pg, q_stats = augment_cardinalities(schema, all_MSCN_est, query_plan_file, table_aliases, target, \n",
    "                                   statistics_file, target_statistics_file)\n",
    "report_stats(est_mscn, est_pg, q_stats)\n",
    "print(len(q_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "231617b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_errors_pg: p50=1.25 p95=26.15 p99=169.25 pmax=533545.23\n",
      "q_errors_mscn: p50=1.25 p95=10.01 p99=63.29 pmax=59107.28\n",
      "59.44% estimated using MSCN\n"
     ]
    }
   ],
   "source": [
    "report_stats(est_mscn, est_pg, q_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d570b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import types\n",
    "import os\n",
    "from json import JSONDecodeError\n",
    "from time import perf_counter\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cross_db_benchmark.benchmark_tools.parse_run import dumper\n",
    "from cross_db_benchmark.benchmark_tools.utils import load_json\n",
    "from models.training.checkpoint import save_csv\n",
    "from data_driven_cardinalities.cardinality_corrector.augment_plan import get_table_aliases_imdb, report_stats, \\\n",
    "    get_act_est_card, q_err\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def augment_cardinalities(schema, src, target, statistics_file, target_statistics_file, tuning_scale=0.5):\n",
    "    try:\n",
    "        run = load_json(src, namespace=True)\n",
    "    except JSONDecodeError:\n",
    "        raise ValueError(f\"Error reading {src}\")\n",
    "\n",
    "\n",
    "    # find out if this a non_inclusive workload (< previously replaced by <=)\n",
    "    non_inclusive = False\n",
    "    if any([b in src for b in ['job-light', 'scale', 'synthetic']]):\n",
    "        non_inclusive = True\n",
    "        print(\"Assuming NON-INCLUSIVE workload\")\n",
    "\n",
    "    q_stats = []\n",
    "    est_pg = 0\n",
    "    est_tuned = 0\n",
    "    for q_id, p in enumerate(tqdm(run.parsed_plans)):\n",
    "        p.plan_parameters.est_pg = 0\n",
    "        p.plan_parameters.est_tuned = 0\n",
    "        augment_bottom_up(schema, p, q_id, q_stats, non_inclusive=non_inclusive, tuning_scale=tuning_scale)\n",
    "        est_pg += p.plan_parameters.est_pg\n",
    "        est_tuned += p.plan_parameters.est_tuned\n",
    "\n",
    "        def augment_prod(p):\n",
    "            if len(p.children) == 0:\n",
    "                p.plan_parameters.tuned_est_children_card = 1\n",
    "            else:\n",
    "                child_card = 1\n",
    "                for c in p.children:\n",
    "                    child_card *= c.plan_parameters.tuned_est_card\n",
    "                    augment_prod(c)\n",
    "                p.plan_parameters.tuned_est_children_card = child_card\n",
    "\n",
    "        augment_prod(p)\n",
    "\n",
    "    argumented_queries = types.SimpleNamespace()\n",
    "    argumented_queries.database_stats = run.database_stats\n",
    "    argumented_queries.run_kwargs = run.run_kwargs\n",
    "    argumented_queries.parsed_plans = []\n",
    "    for q_id, p in enumerate(run.parsed_plans):\n",
    "        if q_id in all_MSCN_est:\n",
    "            argumented_queries.parsed_plans.append(p)\n",
    "\n",
    "    print(len(argumented_queries.parsed_plans))\n",
    "    target_dir = os.path.dirname(target)\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    with open(target, 'w') as outfile:\n",
    "        json.dump(argumented_queries, outfile, default=dumper)\n",
    "\n",
    "    feature_statistics = load_json(statistics_file, namespace=False)\n",
    "    feature_statistics['est_tuned'] = {'max': 0.0, 'scale': 1.0, 'center': 1.0, 'type': 'numeric'}\n",
    "    feature_statistics['tuned_est_card'] = feature_statistics['act_card']\n",
    "    feature_statistics['tuned_est_children_card'] = feature_statistics['act_children_card']\n",
    "    with open(target_statistics_file, \"w\") as f:\n",
    "        json.dump(feature_statistics, f)\n",
    "    return q_stats\n",
    "\n",
    "\n",
    "def augment_bottom_up(schema, plan, q_id, q_stats, non_inclusive=False, tuning_scale=0.5):\n",
    "\n",
    "    for c in plan.children:\n",
    "        augment_bottom_up(schema, c, q_id, q_stats, non_inclusive=non_inclusive, tuning_scale=tuning_scale)\n",
    "\n",
    "    # evaluate query\n",
    "    act_card, pg_est_card = get_act_est_card(plan.plan_parameters)\n",
    "\n",
    "    if act_card is None or pg_est_card is None:\n",
    "        tuned_est = 1\n",
    "        q_err_pg = 1\n",
    "        q_err_tuned = 1\n",
    "    else:\n",
    "        tuned_est = act_card * abs(tuning_scale) + pg_est_card * (1 - tuning_scale)\n",
    "        q_err_pg = q_err(pg_est_card, act_card)\n",
    "        q_err_tuned = q_err(tuned_est, act_card)\n",
    "\n",
    "    q_stats.append({\n",
    "        'query_id': q_id,\n",
    "        'q_errors_pg': q_err_pg,\n",
    "        'q_errors_tuned': q_err_tuned\n",
    "    })\n",
    "\n",
    "    plan.plan_parameters.tuned_est_card = tuned_est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "959f5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 2029/2029 [00:00<00:00, 30901.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598\n"
     ]
    }
   ],
   "source": [
    "tuning_scale=-2\n",
    "query_plan_file = '/home/ziniuw/zero-shot-data/runs/parsed_plans/imdb_full/complex_queries_testing_2k.json'\n",
    "target = f'/home/ziniuw/zero-shot-data/runs/MSCN_augmented/imdb_full/complex_queries_testing_2k_tuned_{tuning_scale}.json'\n",
    "statistics_file = \"/home/ziniuw/zero-shot-data/runs/parsed_plans/statistics_workload_combined.json\"\n",
    "target_statistics_file = '/home/ziniuw/zero-shot-data/runs/tuned_augmented/statistics_workload_combined.json'\n",
    "q_stats = augment_cardinalities(schema, query_plan_file, target, \n",
    "                                   statistics_file, target_statistics_file, tuning_scale=tuning_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32c90297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_errors_pg: p50=1.07 p95=11.01 p99=125.37 pmax=533545.23\n",
      "q_errors_tuned: p50=5.00 p95=14.55 p99=95.70 pmax=1600637.69\n",
      "19313\n"
     ]
    }
   ],
   "source": [
    "def report_stats(q_stats):\n",
    "    if len(q_stats) > 0:\n",
    "        def report_percentiles(key):\n",
    "            vals = np.array([q_s[key] for q_s in q_stats])\n",
    "            print(f\"{key}: p50={np.median(vals):.2f} p95={np.percentile(vals, 95):.2f} \"\n",
    "                  f\"p99={np.percentile(vals, 99):.2f} pmax={np.max(vals):.2f}\")\n",
    "\n",
    "        report_percentiles('q_errors_pg')\n",
    "        report_percentiles('q_errors_tuned')\n",
    "report_stats(q_stats)\n",
    "print(len(q_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92ab5a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_cc_est_best_config.json\"\n",
    "file = load_json(hyperparameter_path, namespace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60f0ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "file['plan_featurization_name'] = \"PostgresTunedCardDetail\"\n",
    "target_hyperparameter_path = \"/home/ziniuw/zero-shot-cost-estimation/setup/tuned_hyperparameters/tune_tuned_est_best_config.json\"\n",
    "with open(target_hyperparameter_path, \"w\") as f:\n",
    "    json.dump(file, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c8f83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
